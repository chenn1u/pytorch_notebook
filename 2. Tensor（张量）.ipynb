{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本期重点：\n",
    "- 2.2 张量的属性\n",
    "- 2.5 数据创建pytorch张量的四种方法的区别\n",
    "- 2.6 张量的重塑操作(reshaping)\n",
    "- 2.8.2 张量的广播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 张量介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 张量是神经网络中使用的主要数据结构，网络中的输入、输出和转换均使用张量表示；\n",
    "* n维张量中，n表示在结构中访问特定元素所需要的索引数量（张量是n维数组）；\n",
    "* 张量被称为一个泛化的原因：我们可以使用张量来表示所有的n维数组；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 索引数量 | 计算机科学中的名称 | 数学中的名称 | Tensor表示|\n",
    "|:-------- |:------------------ |:------------ |:----------|\n",
    "|    0   |      数字      |    标量    |  0维张量  |\n",
    "|    1   |      数组      |    矢量    |  1维张量  |\n",
    "|    2   |    二维数组     |    矩阵    |  2维张量  |\n",
    "|    n   |    N维数组      |   N维张量   | n维张量  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 注意：张量的维度(rank)跟我们所说的向量空间的向量维度不同；张量的维度并不能告诉我们张量中有多少个分量（如果我们在三维的欧几里得空间中有一个三维向量，即我们会有三个分量(x,y,z);而一个三维张量可以有多于三个分量，也可以有少于三个分量）；如下：二维张量t有9个分量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]]\n",
    ")\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量 t 的 rank 是 2，表示我们需要两个所以来访问最里层的内容，shape 是\\[3, 3\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 张量的属性(rank, axes and shape)\n",
    "* 张量的rank, axes, shape是使用张量时最关注的三个属性；\n",
    "* 所有的rank, axes, shape都与索引的概念有本质上的联系；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Rank\n",
    "* 张量的rank是指张量的维数\n",
    "* 一个张量的rank告诉我们需要多少个索引来访问或引用张量结构中包含的特定数据元素\n",
    "\n",
    "### 2.2.2 Axes\n",
    "* 一个张量的axes是一个张量的一个特定维度\n",
    "* 对于张量，其最后一个axes的元素均为数字 \n",
    "* 张量的rank告诉我们一个张量有多少个axes\n",
    "\n",
    "### 2.2.3 Shape\n",
    "* 张量的shape由每个axes的长度决定（知道了张量的shape就可知道每个轴的索引）\n",
    "* 张量的shape很重要：原因1：可以让我们从概念上想象一个张量(越高阶的张量越抽象)；原因2：形状可以提现所有有关rank、axes和索引的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank = len(t.shape)\n",
    "rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t 的第一个 axes 是一个数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t 的第二个 axes 是一个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " t[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 张量输入到神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CNN输入张量的长度通常为4：[batchsize, color_channel, height, width]；通过这4个索引，可以在特定图像的特定颜色通道中导航到特定的像素；\n",
    "* 卷积神经网络的样本输入通常是批量的而不是单个的；\n",
    "* 张量经过卷积层后的变化：卷积会改变高度、宽度以及颜色通道的数量；通道数与滤波器的数量有关；滤波器的大小会影响到高度和宽度；\n",
    "* 经过卷积的通道不再叫彩色通道(已被改变)，而叫做特征通道(特征图:输入颜色通道和卷积滤波器所产生的卷积结果)\n",
    "* 输出通道 = 特征通道 = 特征映射"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 pytorch张量及其创建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pytorch神经网络中必须编写的第一行程序是“数据预处理程序”\n",
    "* 数据预处理的最终目标：是将我们要处理的任何数据转换成能够感知神经网络的张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 张量元素的属性\n",
    "| Data type          |   dtype     |   CPU tensor   |   GPU tensor  |\n",
    "|:-----------------------|:---------------|:-----------------|:---------------|\n",
    "| 32-bit floating point  | torch.float32 | torch.FloatTensor | torch.cuda.FloatTensor |\n",
    "| 64-bit floating point  | torch.float64 | torch.DoubleTensor| torch.cuda.DoubleTensor|\n",
    "| 16-bit floating point  | torch.float16 | torch.HalfTensor  | torch.cuda.HalfTensor |\n",
    "| 8-bit integer(unsigned)| torch.uint8  | torch.BYteTensor| torch.cuda.ByteTensor|\n",
    "| 8-bit integer(signed)  | torch.int8| torch.CharTensor| torch.cuda.CharTensor|\n",
    "| 16-bit integer(signed) | torch,int16|torch.ShortTensor|torch.cuda.ShortTensor|\n",
    "| 32-bit integer(signed) | torch.int32|torch.IntTensor| torch.cuda.IntTensor|\n",
    "| 64-bit integer(signed) | torch.int64|torch.LongTensor| torch.cuda.LongTensor|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "cpu\n",
      "torch.strided\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor(\n",
    "    [[1,2,3],\n",
    "     [4,5,6],\n",
    "     [7,8,9]]\n",
    ")\n",
    "print(t.dtype)       \n",
    "print(t.device)    \n",
    "print(t.layout)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量与张量之间的运算必须是相同数据类型在相同的设备上发生的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 关于张量的两件事\n",
    "* 张量是包含一个同一类型的数据\n",
    "* 张量之间的计算依赖于类型和设备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 用数据创建pytorch张量的4种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "t = np.array(\n",
    "    [[1,2,3],\n",
    "     [4,5,6],\n",
    "     [7,8,9]]\n",
    ")\n",
    "print(torch.Tensor(t))           # 类构造函数\n",
    "print(torch.tensor(t))           # 工厂函数\n",
    "print(torch.as_tensor(t))        # 工厂函数\n",
    "print(torch.from_numpy(t))       # 工厂函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 工厂函数：接受参数输入并返回特定类型对象的函数；\n",
    "* 工厂函数允许更多的动态对象创建；具有更好的文档，并有更多的配置参数；\n",
    "* 通常情况下会更倾向于选择工厂函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 无数据情况下创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9620, 0.4890, 0.6287],\n",
       "        [0.4797, 0.9278, 0.3500],\n",
       "        [0.8081, 0.7143, 0.9540]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 数据创建pytorch张量的四种方法的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pytorch张量是torch.tensor pytorch类的实例\n",
    "* 一个张量的抽象概念和一个pytorch张量的区别在于：pytorch张量给了我们一个具体的实现，我们可以使用和编码它"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 四种方法的区别1：数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "torch.float32\n",
      "----------------\n",
      "tensor([1, 2, 3])\n",
      "torch.int64\n",
      "----------------\n",
      "tensor([1, 2, 3])\n",
      "torch.int64\n",
      "----------------\n",
      "tensor([1, 2, 3])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "data = np.array([1,2,3])\n",
    "t1 = torch.Tensor(data)\n",
    "print(t1)\n",
    "print(t1.dtype)\n",
    "print('----------------')\n",
    "t2 = torch.tensor(data)\n",
    "print(t2)\n",
    "print(t2.dtype)\n",
    "print('----------------')\n",
    "t3 = torch.as_tensor(data)\n",
    "print(t3)\n",
    "print(t3.dtype)\n",
    "print('----------------')\n",
    "t4 = torch.from_numpy(data)\n",
    "print(t4)\n",
    "print(t4.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 上述使用类构造函数和工厂函数后生成的数据类型不同，主要原因是：构造函数在构造一个张量时使用**全局缺省值**，而工厂函数通过输入数据的类型来**推断**输出数据的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 工厂函数可以显示指定数据类型，类构造函数不能这样操作\n",
    "t = torch.tensor(np.array([1,2,3]), dtype=torch.float64)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 四种方法的区别2：数据分配内存方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([0, 0, 0])\n",
      "tensor([0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "data = np.array([1,2,3])\n",
    "print(data)\n",
    "\n",
    "t1 = torch.Tensor(data)\n",
    "t2 = torch.tensor(data)\n",
    "t3 = torch.as_tensor(data)\n",
    "t4 = torch.from_numpy(data)\n",
    "\n",
    "data[0] = 0\n",
    "data[1] = 0\n",
    "data[2] = 0\n",
    "# t1 和 t2 输出的都是更改前的数组\n",
    "print(t1)\n",
    "print(t2)\n",
    "# t3 和 t4 输出的都是更改后的数组\n",
    "print(t3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 上述差异是由创建时分配内存的方式造成的：t1和t2的方式是将数组中的元素值直接拷贝到张量中，改变data中的元素值并不会影响到t1和t2中的值；t3和t4的方式是与data数组共享数据；（可将t1和t2的方式看作\"值传递\"；t3和t4的方式看作“地址传递”）\n",
    "|    共享数据    |    数据拷贝   |\n",
    "|:-----------------|:----------------|\n",
    "| torch.as_tensor() | torch.tensor()|\n",
    "| torch.from_numpy()| torch.Tensor()|\n",
    "* 由上可知numpy和tensor是数据共享的，所以他们可以无缝切换\n",
    "* 数据共享比数据拷贝更高效，更节省内存空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3 最优的数据转换方法\n",
    "* 数据拷贝方式的最优选择是 torch.tensor() (因为是工厂函数)\n",
    "* 内存共享方式的最优选择是 torch.as_tensor() (因为torch.as_tensor可以接受任何python数据结构；而torch.from_numpy只接受numpy数组)\n",
    "* 数据拷贝的方式更注重实现；而内存共享的方式更注重代码性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.4 使用内存共享工厂函数的注意事项：\n",
    "* 1.由于numpy.ndaaray对象分配在CPU上，所以如果使用GPU的话，torch.as_tensor函数必须把数据从CPU上拷到GPU上\n",
    "* 2.as_tensor()对于python内置的数据结构，如列表，是无效的\n",
    "* 3.as_tensor的调用要求熟悉共享特征，以免对底层数据做不必要的更改，而影响到对象\n",
    "* 4.当as_tensor和numpy.ndarray有大量的相互往返的操作时，对性能的提升会有较大的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 张量的重塑操作(reshaping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 常用张量操作类型\n",
    "* Reshaping operations\n",
    "* Element-wise operations\n",
    "* Reduction operations\n",
    "* Access operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 张量的重塑\n",
    "* 张量的重塑是最重要的张量操作：因为张量的形状能提供给我们一些具体的东西，我们可以用它来塑造和直观的理解张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor(\n",
    "    [[1,1,1,1],\n",
    "     [2,2,2,2],\n",
    "     [3,3,3,3]], dtype=torch.float32)\n",
    "print(t.size())\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到张量中的元素个数\n",
    "t.numel() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元素的个数与张量的重塑有直接的关系，重塑后每个轴长之积必须与元素个数相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n",
      "torch.Size([1, 12])\n",
      "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])\n",
      "torch.Size([12])\n",
      "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n",
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "# reshape, squeezing\n",
    "print(t.reshape(1,12))\n",
    "print(t.reshape(1,12).shape)\n",
    "# squeezing\n",
    "print(t.reshape(1,12).squeeze())\n",
    "print(t.reshape(1,12).squeeze().shape)\n",
    "# unsqueezing\n",
    "print(t.reshape(1,12).squeeze().unsqueeze(dim=0))\n",
    "print(t.reshape(1,12).squeeze().unsqueeze(dim=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* squeezing:可以移除所有长度为1的axes (减少rank)\n",
    "* unsqueezing: 会增加一个长度为1的axes （增加rank）\n",
    "* flatten张量：除去所有的axes，只保留一个，创造一个单轴张量包含原张量所有元素；\n",
    "* flatten操作是从一个卷积层过度到一个全连接层时在神经网络中必须发生的；\n",
    "* flatten操作是一种特殊的reshaping操作，即所有axes被挤压成一个axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张量的flatten\n",
    "t = torch.tensor(\n",
    "    [[1,1,1,1],\n",
    "     [2,2,2,2],\n",
    "     [3,3,3,3]], dtype=torch.float32)\n",
    "t = t.flatten()   \n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 多批次张量的flatten\n",
    "* flatten操作的前提：张量至少有两个轴\n",
    "* 全连接层接收被flatten的张量来作为输入\n",
    "* 多批次张量的flatten：保持batch的长度不变，只flatten图像通道部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 4])\n",
      "tensor([[[1, 1, 1, 1],\n",
      "         [1, 1, 1, 1],\n",
      "         [1, 1, 1, 1],\n",
      "         [1, 1, 1, 1]]])\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1]])\n",
      "tensor([1, 1, 1, 1])\n",
      "tensor(1)\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n",
      "torch.Size([3, 16])\n"
     ]
    }
   ],
   "source": [
    "# t1,t2,t3为三个4x4的图像\n",
    "t1 = torch.tensor([\n",
    "            [1,1,1,1],\n",
    "            [1,1,1,1],\n",
    "            [1,1,1,1],\n",
    "            [1,1,1,1]\n",
    "        ])\n",
    "\n",
    "t2 = torch.tensor([\n",
    "            [2,2,2,2],\n",
    "            [2,2,2,2],\n",
    "            [2,2,2,2],\n",
    "            [2,2,2,2]\n",
    "        ])\n",
    "\n",
    "t3 = torch.tensor([\n",
    "            [3,3,3,3],\n",
    "            [3,3,3,3],\n",
    "            [3,3,3,3],\n",
    "            [3,3,3,3]\n",
    "        ])\n",
    "# 使用stack将其合并成一个秩为3的张量\n",
    "t = torch.stack((t1,t2,t3))\n",
    "print(t.shape)    \n",
    "# 增加一个彩色通道轴，将其变成CNN期望的形式，以上三张图像均为灰度图像\n",
    "t = t.reshape(3,1,4,4)\n",
    "print(t[0])    # 第一个图像\n",
    "print(t[0][0])     # 第一个图像的第一个通道\n",
    "print(t[0][0][0])  # 第一个图像的第一个通道中的第一行像素\n",
    "print(t[0][0][0][0])   # 第一个图像的第一个通道中的第一行的第一个像素\n",
    "\n",
    "# 该张量的flatten:我们期望只将图像张量flatten，而不是全部flatten\n",
    "# 参数start_dim告诉flatten函数应该从哪个轴开始\n",
    "print(t.flatten(start_dim=1))  \n",
    "print(t.flatten(start_dim=1).shape)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 张量的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8.1 张量的元素运算(element-wise operation)\n",
    "* 元素运算是对张量元素的运算，这些元素在张量中对应或具有相同的位置索引\n",
    "* 两个张量必须有相同的形状才能执行元素操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "                [1,2],\n",
    "                [3,4]\n",
    "            ], dtype=torch.float32)\n",
    "\n",
    "t2 = torch.tensor([\n",
    "                [9,8],\n",
    "                [7,6]\n",
    "            ], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10.],\n",
       "        [10., 10.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7., 6.],\n",
       "        [5., 4.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 4.],\n",
       "        [6., 8.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 1.0000],\n",
       "        [1.5000, 2.0000]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 /2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8.2 张量的广播\n",
    "* 张量的广播：定义了在元素操作的过程中如何处理不同形状的张量，即：将标量变成与另一个张量相同的形状\n",
    "* 张量广播在数据标准化时会常用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 5],\n",
      "        [3, 5]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([\n",
    "                [1,1],\n",
    "                [1,1]\n",
    "            ])\n",
    "t2 = torch.tensor([2,4])\n",
    "print(t1+t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 5],\n",
       "        [3, 5]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast_to(t2.numpy,t1.shape) #广播\n",
    "t1 + t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8.3 张量的比较运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 会返回一个与比较张量形状相同的张量，其值为0或1（0表示False；1表示True）\n",
    "* element-wise,component-wise以及point-wise的操作方式均相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 张量的缩减操作（reduction）\n",
    "* 缩减操作是一个减少张量中包含的元素数量的操作\n",
    "* 元素操作允许我们对多个张量进行操作；缩减操作允许我们对单个张量进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.)\n",
      "9\n",
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "                [0,1,0],\n",
    "                [2,0,2],\n",
    "                [0,3,0]\n",
    "            ], dtype = torch.float32)\n",
    "print(t.sum()) \n",
    "print(t.numel())  \n",
    "print(t.sum().numel()) \n",
    "print(t.sum().numel() < t.numel())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上可知，求和操作是缩减操作，缩减操作还有：\n",
    "* t.sum()\n",
    "* t.prod()\n",
    "* t.mean()\n",
    "* t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 6., 6., 6.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 缩减操作通常允许跨数据结构计算总值\n",
    "# 对某个轴进行缩减操作\n",
    "t = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)\n",
    "t.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.,  8., 12.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* argmax函数：得到张量的最大输出值的对应索引位置\n",
    "* 在实际应用中，经常在神经网络输出预测张量上使用argmax，确定哪个类别的预测最高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "                [1,0,0,2],\n",
    "                [0,3,3,0],\n",
    "                [4,0,0,5]\n",
    "            ], dtype=torch.float32)\n",
    "t.max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.argmax()            # 输出的是flatten后的索引"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
